{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from bertopic import BERTopic\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from umap import UMAP\n",
    "from hdbscan import HDBSCAN\n",
    "import numpy as np\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "cases_df = pd.read_csv('../cases.csv')\n",
    "cases_sample = cases_df[:10].dropna(subset=['body']).fillna('')\n",
    "# print(len(cases_sample))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# remove URLs and special texts\n",
    "def clean_text(text):\n",
    "    text = re.sub(r'https?://\\S+', '', text)\n",
    "    text = re.sub(r'[^A-Za-z0-9\\s.,?]', '', text)\n",
    "    text = text.strip().lower()\n",
    "\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "documents = []\n",
    "for index, row in cases_sample.iterrows():\n",
    "    joined_text = ' '.join([(row['title']), (row['description']), (row['body'])])\n",
    "    doc = clean_text(joined_text)\n",
    "    \n",
    "    documents.append(doc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding_model = SentenceTransformer(\"all-MiniLM-L6-v2\")\n",
    "embeddings = embedding_model.encode(documents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "umap_model = UMAP(n_neighbors=15, n_components=5, min_dist=0.0, metric='cosine', random_state=42)\n",
    "vectorizer_model = CountVectorizer(stop_words=\"english\", min_df=2, ngram_range=(1, 2))\n",
    "vectorizer_model = CountVectorizer(stop_words=\"english\")\n",
    "# hdbscan_model = HDBSCAN(min_cluster_size=150, metric='euclidean', cluster_selection_method='eom', prediction_data=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "topic_model = BERTopic(\n",
    "    embedding_model=embedding_model, \n",
    "    umap_model=umap_model,\n",
    "    vectorizer_model=vectorizer_model,\n",
    "    top_n_words=10,\n",
    "    verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-02-17 16:46:29,739 - BERTopic - Dimensionality - Fitting the dimensionality reduction algorithm\n",
      "2025-02-17 16:46:29,755 - BERTopic - Dimensionality - Completed ✓\n",
      "2025-02-17 16:46:29,755 - BERTopic - Cluster - Start clustering the reduced embeddings\n",
      "2025-02-17 16:46:29,757 - BERTopic - Cluster - Completed ✓\n",
      "2025-02-17 16:46:29,759 - BERTopic - Representation - Extracting topics from clusters using representation models.\n",
      "2025-02-17 16:46:29,777 - BERTopic - Representation - Completed ✓\n"
     ]
    }
   ],
   "source": [
    "topics, probs = topic_model.fit_transform(documents, embeddings)\n",
    "embeddings = topic_model._extract_embeddings(documents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.save('sample_embeddings.npy', embeddings)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
